---
title: "[음성분석] 너의 소울을 들려줘: 랩 시뮬레이터"
order: 4
thumbnail: assets/img/thumbs/rap_simulator.png
tags: [ML, DL, Voice, Education, Transformer]
links:
  repo: https://github.com/YOUR/repo1
  report: /assets/docs/umnd-report.pdf
---

# 🎤 랩 가수 음성 분석 및 유사 아티스트 추천 시스템
랩 가수의 음성적 특성과 감정적 톤을 수치화하여 지망생에게 본인과 유사한 프로 래퍼를 추천하는 딥러닝 프로젝트

## 주요 성과
- 랩 가수의 음성 특징을 추출하여 지망생의 독학에 도움을 주는 랩 시뮬레이터 모델 개발
  * 🎧 **음성 기반 래퍼 스타일 임베딩 모델 개발**

  * MFCC, Spectral Contrast, Zero Crossing Rate 등 다차원 피처를 활용해 음성 패턴을 추출.
  * Siamese 구조를 통해 래퍼 간 거리 기반 유사도 계산 모델 구현.
* 🔍 **설명 가능한 추천 시스템(XAI) 구현**

  * SHAP 분석을 통해 “왜 이 래퍼가 유사한가?”를 시각적으로 설명.
  * 톤, 템포, 리듬, 감정 에너지 등 주요 특징별 기여도 시각화.
* 🧠 **데이터 불균형 및 음질 편차 극복**

  * 타임스트레칭, 피치시프트, SpecAugment로 데이터 증강.
  * 마이크·녹음 환경 편차를 보정하여 현실적 입력 대응력 확보.
* 💡 **응용 가능성 제시**

  * AI 보컬 분석, 음악 추천, 음성 창작 지원 등 창작 도메인으로 확장 가능성을 확인.


## 개요
6인 팀 프로젝트를 진행하며, 음성 데이터 전처리, 모델링, 시각화를 담당하였습니다.

1. Transformer 활용하여 HuBERT 모델 인코딩 및 파인튜닝
2. 샴 네트워크 구현

---

## 🎤 래퍼 음성 분석 및 유사 래퍼 추천 시스템

>

---

### 📌 개요

랩은 언어, 리듬, 감정이 복합적으로 얽힌 예술 형식으로, 단순한 음성 인식 기술로는 표현의 개성을 포착하기 어렵다.
이 프로젝트는 음성 신호에서 **‘스타일적 감정’을 수치화**하기 위해 **Siamese Network + Contrastive Loss** 구조를 활용했다.
단순한 분류가 아닌, 각 래퍼의 고유한 **음성 스타일 임베딩 공간**을 구축하여,
지망생 래퍼가 자신의 목소리로 녹음한 샘플을 업로드하면 **유사한 프로 래퍼를 추천**받을 수 있도록 설계했다.

---

### 🚀 주요 성과



---

### 🔍 상세 내용

1. **데이터 수집 및 전처리**

   * Jay-Z, Eminem, Snoop Dogg 등 주요 래퍼 음성 클립 1만여 개 수집.
   * Librosa를 이용해 비트 단위 구간 정규화 및 특징 벡터화(MFCC·Chroma·Tempo).
   * 녹음 품질·환경 편차 완화를 위해 band-noise filtering, SpecAugment 적용.

2. **모델링**

   * Siamese Network로 구성된 **Contrastive Loss 기반 구조** 설계.

     * 동일 래퍼: 임베딩 거리 축소
     * 상이 래퍼: 거리 확장
   * Latent embedding을 이용해 **유사 래퍼 추천** 알고리즘 구성.

     * 지망생 랩 입력 → 임베딩 추출 → 코사인 거리 계산 → Top-k 래퍼 추천

3. **모델 해석(XAI)**

   * SHAP을 통해 모델이 유사도를 판단할 때 어떤 음향 특성이 가장 크게 작용했는지 분석.
   * 예: 지망생의 음성에서 “에너지 레벨↑ + 낮은 MFCC 대역”이 Eminem과 유사하다고 판단.

4. **결과 시각화**

   * t-SNE로 임베딩 공간 시각화: 래퍼별 클러스터링 확인.
   * 유사도 매트릭스 기반 Heatmap으로 스타일 관계를 한눈에 표현.

---

### 💭 느낀 점

이 프로젝트를 통해 단순한 분류 정확도보다 **‘해석 가능한 감각적 이해’**가 얼마나 중요한지를 깨달았다.
음성 데이터는 단순한 신호가 아니라, **인간의 개성과 정체성이 투영된 데이터**라는 점에서
AI가 예술의 영역에 접근하려면 ‘정확도’보다 ‘이해 가능성’이 먼저여야 함을 체감했다.
결국 이 실험은 모델 성능을 높이기보다, **기계가 인간의 감각을 어떻게 모사할 수 있는가**를 탐구한 시도였다.
💿 “음악은 데이터를 넘어, 인간의 흔적이다” — 이 신념이 프로젝트 전체를 관통했다.

---

원하면 이걸 **포트폴리오 README용 요약 버전(한 화면에 들어가는 카드형)**으로 줄여줄 수도 있어.
예를 들어 “AI Rap Style Recommender 🎤 (1페이지 요약)”처럼. 그렇게 만들어줄까?



## 기술스택
Python(pandas, scikit-learn, pytorch)


## 링크
- 코드: [GitHub]({{ page.links.repo }})
- 보고서: [PDF]({{ page.links.report }})
